{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c512582",
   "metadata": {},
   "source": [
    "\n",
    "# ETL Foreign Exchange Rates from Bangko Sentral ng Pilipinas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3105d93",
   "metadata": {},
   "source": [
    "## Todos:\n",
    "\n",
    "1. Create a python script that will scrape exchange rate conversions from [Bangko Sentral ng Pilipinas](https://www.bsp.gov.ph/SitePages/Statistics/ExchangeRate.aspx) official website.\n",
    "2. Save the raw unprocessed scraped data to `data/raw/rates` and name it as **rates_[timestamp].json**.\n",
    "2. Use **pyspark** to do cleansing and transformation against the scraped data.\n",
    "3. Save the processed data as **rates.json** to `data/processed/rates` folder.\n",
    "4. Use `Postgres` db to store and analyze processed foreign exchange rates.\n",
    "5. Schedule the ETL task to run on a daily basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322c144",
   "metadata": {},
   "source": [
    "## Prerequisites:\n",
    "\n",
    "- **Selenium** is going to be used for automating browser operations.\n",
    "- **WebDriver Manager (_optional_)** will provide us the driver we need to work with selenium.\n",
    "- **BeautifulSoup** will parse the html markup to be generated by selenium.\n",
    "- **PySpark** will be used for data wrangling, cleansing and transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f459671",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "\n",
    "You can install required packages by running the ff. command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3729c8",
   "metadata": {},
   "source": [
    "- Run from a terminal:\n",
    "\n",
    "```\n",
    "    pip install webdriver_manager bs4 selenium\n",
    "```\n",
    "\n",
    "- Run from a notebook:\n",
    "\n",
    "```\n",
    "    !pip install webdriver_manager bs4 selenium\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0404e",
   "metadata": {},
   "source": [
    "## Creating the scraping tool using python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3e339",
   "metadata": {},
   "source": [
    "Start by importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f3bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party libraries\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Local imports\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b30587",
   "metadata": {},
   "source": [
    "This one is optional, but if you want your browser to run only in the background, add the following lines of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5783d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.headless = True # This option will hide your browsers user interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c50bd",
   "metadata": {},
   "source": [
    "The following lines of code will do the ff:\n",
    "\n",
    "1. Starts up a browser that will automatically redirect to **Bangko Sentral ng Pilipinas** official website.\n",
    "2. Captures the website's page source and then stores it to `html` variable.\n",
    "3. BeautifulSoup will then parse the value stored in `html`.\n",
    "4. Uses the `.find()` method to search for the web element(s) which contains the data we needed to extract.\n",
    "5. Saves the extracted data inside `data/raw` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c26c80cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\john.delmundo\\AppData\\Local\\Temp\\ipykernel_27212\\1261582902.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://www.bsp.gov.ph/SitePages/Statistics/ExchangeRate.aspx\"\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "driver.get(URL)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "exchange_rate_table = soup.find(id=\"tb1\")\n",
    "table2 = soup.find(id=\"tb2\")\n",
    "\n",
    "headers = [\n",
    "    \"country\",\n",
    "    \"unit\",\n",
    "    \"symbol\",\n",
    "    \"euro_equivalent\",\n",
    "    \"us_dollar_equivalent\",\n",
    "    \"phil_peso_equivalent\"\n",
    "]\n",
    "\n",
    "row = []\n",
    "rates = []\n",
    "\n",
    "for index, _ in enumerate(exchange_rate_table.contents):\n",
    "    row.clear()\n",
    "    if index > 0:\n",
    "        for index, tag in enumerate(_.contents):\n",
    "            row.append(tag.text)\n",
    "        rates.append(list(row))\n",
    "\n",
    "# Appends row headers at the beginning of the list\n",
    "rates.insert(0, headers) \n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%T\").replace(\":\", \"\")\n",
    "\n",
    "with open(f\"data/raw/rates_{timestamp}.csv\", \"w\", newline=\"\\n\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b577c742",
   "metadata": {},
   "source": [
    "## Create a spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38a1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ETL Foreign Exchange Rates from Bangko Sentral ng Pilipinas\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e5c4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x203bac7e1a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkConf().set(\"spark.app.name\", \"ETL Foreign Exchange Rates from Bangko Sentral ng Pilipinas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec126289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ETL Foreign Exchange Rates from Bangko Sentral ng Pilipinas'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.app.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec191cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40915dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MIS-JPDelmundo.CEGROUP.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL Foreign Exchange Rates from Bangko Sentral ng Pilipinas</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x203c5696680>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19229c7a",
   "metadata": {},
   "source": [
    "## Define the structure of the schema\n",
    "\n",
    "In here, we use the **StructType** and **StructField** python classes to define the structure of our schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "561eeeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+---------------+--------------------+--------------------+\n",
      "|             country|    unit|symbol|euro_equivalent|us_dollar_equivalent|phil_peso_equivalent|\n",
      "+--------------------+--------+------+---------------+--------------------+--------------------+\n",
      "|     1 UNITED STATES|  DOLLAR|   USD|       0.924642|                 1.0|              55.797|\n",
      "|             2 JAPAN|     YEN|   JPY|       0.006673|            0.007217|              0.4027|\n",
      "|    3 UNITED KINGDOM|   POUND|   GBP|       1.149977|              1.2437|             69.3947|\n",
      "|          4 HONGKONG|  DOLLAR|   HKD|        0.11812|            0.127747|              7.1279|\n",
      "|       5 SWITZERLAND|   FRANC|   CHF|       1.030241|            1.114206|             62.1694|\n",
      "|            6 CANADA|  DOLLAR|   CAD|       0.684971|            0.740796|             41.3342|\n",
      "|         7 SINGAPORE|  DOLLAR|   SGD|       0.687006|            0.742997|              41.457|\n",
      "|         8 AUSTRALIA|  DOLLAR|   AUD|       0.614794|              0.6649|             37.0994|\n",
      "|           9 BAHRAIN|  DINAR*|   BHD|       2.452826|            2.652731|            148.0144|\n",
      "|           10 KUWAIT|   DINAR|   KWD|           null|                null|                null|\n",
      "|     11 SAUDI ARABIA|   RIYAL|   SAR|       0.246578|            0.266674|             14.8796|\n",
      "|           12 BRUNEI|  DOLLAR|   BND|       0.684463|            0.740247|             41.3036|\n",
      "|        13 INDONESIA|  RUPIAH|   IDR|         6.2E-5|              6.7E-5|              0.0037|\n",
      "|         14 THAILAND|BAHT****|   THB|       0.026855|            0.029044|              1.6206|\n",
      "|15 UNITED ARAB EM...|  DIRHAM|   AED|       0.251829|            0.272353|             15.1965|\n",
      "|16 EUROPEAN MONET...|    EURO|   EUR|            1.0|              1.0815|             60.3445|\n",
      "|            17 KOREA|     WON|   KRW|        7.05E-4|             7.62E-4|              0.0425|\n",
      "|            18 CHINA|  YUAN**|   CNY|       0.131513|            0.142231|              7.9361|\n",
      "+--------------------+--------+------+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    StructField, \n",
    "    FloatType,\n",
    "    StringType\n",
    ")\n",
    "\n",
    "path = \"data/raw/rates_20230523_173600.csv\"\n",
    "\n",
    "struct = StructType([\n",
    "    StructField(\"country\", StringType(), False),\n",
    "    StructField(\"unit\", StringType(), False),\n",
    "    StructField(\"symbol\", StringType(), False),\n",
    "    StructField(\"euro_equivalent\", FloatType(), False),\n",
    "    StructField(\"us_dollar_equivalent\", FloatType(), False),\n",
    "    StructField(\"phil_peso_equivalent\", FloatType(), False)\n",
    "])\n",
    "\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .schema(struct) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(path)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42def7b2",
   "metadata": {},
   "source": [
    "## Verify the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f04b22ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- euro_equivalent: float (nullable = true)\n",
      " |-- us_dollar_equivalent: float (nullable = true)\n",
      " |-- phil_peso_equivalent: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd9534",
   "metadata": {},
   "source": [
    "## Viewing data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bcdf77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------+------+------------------+--------------------+--------------------+\n",
      "|summary|        country|    unit|symbol|   euro_equivalent|us_dollar_equivalent|phil_peso_equivalent|\n",
      "+-------+---------------+--------+------+------------------+--------------------+--------------------+\n",
      "|  count|             18|      18|    18|                17|                  17|                  17|\n",
      "|   mean|           null|    null|  null|0.5888973548456252|  0.6368924719255815|   35.53669452802826|\n",
      "| stddev|           null|    null|  null|0.6276651628587195|  0.6788198137459448|   37.87610591694862|\n",
      "|    min|1 UNITED STATES|BAHT****|   AED|            6.2E-5|              6.7E-5|              0.0037|\n",
      "|    25%|           null|    null|  null|           0.11812|            0.127747|              7.1279|\n",
      "|    50%|           null|    null|  null|          0.614794|              0.6649|             37.0994|\n",
      "|    75%|           null|    null|  null|          0.924642|                 1.0|              55.797|\n",
      "|    max|      9 BAHRAIN|  YUAN**|   USD|          2.452826|            2.652731|            148.0144|\n",
      "+-------+---------------+--------+------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9a35a",
   "metadata": {},
   "source": [
    "## Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1fa3d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             country|\n",
      "+--------------------+\n",
      "| [1, UNITED, STATES]|\n",
      "|          [2, JAPAN]|\n",
      "|[3, UNITED, KINGDOM]|\n",
      "|       [4, HONGKONG]|\n",
      "|    [5, SWITZERLAND]|\n",
      "|         [6, CANADA]|\n",
      "|      [7, SINGAPORE]|\n",
      "|      [8, AUSTRALIA]|\n",
      "|        [9, BAHRAIN]|\n",
      "|        [10, KUWAIT]|\n",
      "| [11, SAUDI, ARABIA]|\n",
      "|        [12, BRUNEI]|\n",
      "|     [13, INDONESIA]|\n",
      "|      [14, THAILAND]|\n",
      "|[15, UNITED, ARAB...|\n",
      "|[16, EUROPEAN, MO...|\n",
      "|         [17, KOREA]|\n",
      "|         [18, CHINA]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df1 = df.select(\n",
    "    split(\"country\", \" \", -1).alias(\"country\"),\n",
    "    \"unit\",\n",
    "    \"symbol\",\n",
    "    \"euro_equivalent\",\n",
    "    \"us_dollar_equivalent\",\n",
    "    \"phil_peso_equivalent\"\n",
    ")\n",
    "\n",
    "df1.select(\"country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f879f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             country|\n",
      "+--------------------+\n",
      "|    [UNITED, STATES]|\n",
      "|             [JAPAN]|\n",
      "|   [UNITED, KINGDOM]|\n",
      "|          [HONGKONG]|\n",
      "|       [SWITZERLAND]|\n",
      "|            [CANADA]|\n",
      "|         [SINGAPORE]|\n",
      "|         [AUSTRALIA]|\n",
      "|           [BAHRAIN]|\n",
      "|            [KUWAIT]|\n",
      "|     [SAUDI, ARABIA]|\n",
      "|            [BRUNEI]|\n",
      "|         [INDONESIA]|\n",
      "|          [THAILAND]|\n",
      "|[UNITED, ARAB, EM...|\n",
      "|[EUROPEAN, MONETA...|\n",
      "|             [KOREA]|\n",
      "|             [CHINA]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import slice, concat_ws, size\n",
    "\n",
    "df1 = df1.select(\n",
    "    slice(\"country\", 2, size(\"country\")).alias(\"country\"),\n",
    "    \"unit\",\n",
    "    \"symbol\",\n",
    "    \"euro_equivalent\",\n",
    "    \"us_dollar_equivalent\",\n",
    "    \"phil_peso_equivalent\"\n",
    ")\n",
    "\n",
    "df1.select(\"country\").show()\n",
    "\n",
    "df1_view = df1.createOrReplaceTempView(\"exchange_rates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2015789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             country|\n",
      "+--------------------+\n",
      "|       UNITED STATES|\n",
      "|               JAPAN|\n",
      "|      UNITED KINGDOM|\n",
      "|            HONGKONG|\n",
      "|         SWITZERLAND|\n",
      "|              CANADA|\n",
      "|           SINGAPORE|\n",
      "|           AUSTRALIA|\n",
      "|             BAHRAIN|\n",
      "|              KUWAIT|\n",
      "|        SAUDI ARABIA|\n",
      "|              BRUNEI|\n",
      "|           INDONESIA|\n",
      "|            THAILAND|\n",
      "|UNITED ARAB EMIRATES|\n",
      "|EUROPEAN MONETARY...|\n",
      "|               KOREA|\n",
      "|               CHINA|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df1 = df1.select(\n",
    "    concat_ws(\" \", \"country\").alias(\"country\"),\n",
    "    \"unit\",\n",
    "    \"symbol\",\n",
    "    \"euro_equivalent\",\n",
    "    \"us_dollar_equivalent\",\n",
    "    \"phil_peso_equivalent\"\n",
    ")\n",
    "\n",
    "df1.select(\"country\").show()\n",
    "\n",
    "df1_view = df1.createOrReplaceTempView(\"exchange_rates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab9e4aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             country|\n",
      "+--------------------+\n",
      "|       United States|\n",
      "|               Japan|\n",
      "|      United Kingdom|\n",
      "|            Hongkong|\n",
      "|         Switzerland|\n",
      "|              Canada|\n",
      "|           Singapore|\n",
      "|           Australia|\n",
      "|             Bahrain|\n",
      "|              Kuwait|\n",
      "|        Saudi Arabia|\n",
      "|              Brunei|\n",
      "|           Indonesia|\n",
      "|            Thailand|\n",
      "|United Arab Emirates|\n",
      "|European Monetary...|\n",
      "|               Korea|\n",
      "|               China|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "script = \"\"\"SELECT \n",
    "                INITCAP(country) AS country, \n",
    "                unit, \n",
    "                symbol, \n",
    "                euro_equivalent, \n",
    "                us_dollar_equivalent,\n",
    "                phil_peso_equivalent \n",
    "            from \n",
    "                exchange_rates\n",
    "    \"\"\"\n",
    "\n",
    "res = spark.sql(script)\n",
    "res.select(\"country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "631de280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+---------------+--------------------+--------------------+----------+\n",
      "|             country|    unit|symbol|euro_equivalent|us_dollar_equivalent|phil_peso_equivalent| timestamp|\n",
      "+--------------------+--------+------+---------------+--------------------+--------------------+----------+\n",
      "|       United States|  DOLLAR|   USD|       0.924642|                 1.0|              55.797|2023-05-24|\n",
      "|               Japan|     YEN|   JPY|       0.006673|            0.007217|              0.4027|2023-05-24|\n",
      "|      United Kingdom|   POUND|   GBP|       1.149977|              1.2437|             69.3947|2023-05-24|\n",
      "|            Hongkong|  DOLLAR|   HKD|        0.11812|            0.127747|              7.1279|2023-05-24|\n",
      "|         Switzerland|   FRANC|   CHF|       1.030241|            1.114206|             62.1694|2023-05-24|\n",
      "|              Canada|  DOLLAR|   CAD|       0.684971|            0.740796|             41.3342|2023-05-24|\n",
      "|           Singapore|  DOLLAR|   SGD|       0.687006|            0.742997|              41.457|2023-05-24|\n",
      "|           Australia|  DOLLAR|   AUD|       0.614794|              0.6649|             37.0994|2023-05-24|\n",
      "|             Bahrain|  DINAR*|   BHD|       2.452826|            2.652731|            148.0144|2023-05-24|\n",
      "|              Kuwait|   DINAR|   KWD|           null|                null|                null|2023-05-24|\n",
      "|        Saudi Arabia|   RIYAL|   SAR|       0.246578|            0.266674|             14.8796|2023-05-24|\n",
      "|              Brunei|  DOLLAR|   BND|       0.684463|            0.740247|             41.3036|2023-05-24|\n",
      "|           Indonesia|  RUPIAH|   IDR|         6.2E-5|              6.7E-5|              0.0037|2023-05-24|\n",
      "|            Thailand|BAHT****|   THB|       0.026855|            0.029044|              1.6206|2023-05-24|\n",
      "|United Arab Emirates|  DIRHAM|   AED|       0.251829|            0.272353|             15.1965|2023-05-24|\n",
      "|European Monetary...|    EURO|   EUR|            1.0|              1.0815|             60.3445|2023-05-24|\n",
      "|               Korea|     WON|   KRW|        7.05E-4|             7.62E-4|              0.0425|2023-05-24|\n",
      "|               China|  YUAN**|   CNY|       0.131513|            0.142231|              7.9361|2023-05-24|\n",
      "+--------------------+--------+------+---------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    current_date, \n",
    "    current_timestamp\n",
    ")\n",
    "\n",
    "res.select(\"*\").withColumn(\"timestamp\", current_date()).show()\n",
    "res = res.select(\"*\").withColumn(\"timestamp\", current_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06486748",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%T\").replace(\":\", \"\")\n",
    "\n",
    "res.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .save(r\"data\\processed\\rates\\rates_{}\".format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bb3029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesSkills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"languagesSkills\", ArrayType(StringType())),\n",
    "])\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "abb22496",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.extraClassPath\".\nSee also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [222]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.extraClassPath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mjohn.delmundo\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAppData\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mLocal\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPrograms\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPython\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPython310\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mLib\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msite-packages\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpyspark\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mjars\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mxbean-asm9-shaded-4.22.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\conf.py:47\u001b[0m, in \u001b[0;36mRuntimeConfig.set\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, value: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mbool\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124;03m\"\"\"Sets the given Spark runtime configuration property.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 3.4.0\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m        Supports Spark Connect.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.extraClassPath\".\nSee also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'."
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.driver.extraClassPath\", r\"C:\\Users\\john.delmundo\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\pyspark\\jars\\xbean-asm9-shaded-4.22.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f5249efd",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o928.load.\n: java.sql.SQLException: No suitable driver\r\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:300)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [211]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrates\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:postgresql://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatabase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m jdbcDF \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o928.load.\n: java.sql.SQLException: No suitable driver\r\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:300)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n"
     ]
    }
   ],
   "source": [
    "host = \"localhost\"\n",
    "username = \"postgres\"\n",
    "password = \"admin123\"\n",
    "database = \"exchange_rates\"\n",
    "table = \"rates\"\n",
    "\n",
    "url = f\"jdbc:postgresql://{host}/{database}\"\n",
    "\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b3d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
